#!/bin/bash
#
# Run data integrity tests and promote data in staging buckets to production

set -eEuo pipefail

usage() {
cat << EOF

  Run data integrity tests and promote data in staging buckets to production.

  Generates a Markdown report for data integrity. Data cannot be promoted if tests fail.

  $(tput bold)[CAUTION] This also deletes data in the production bucket that is not found in the staging bucket.$(tput sgr0)

  Usage: $0 [OPTIONS]

  OPTIONS
  ───────
    $(tput bold)-h$(tput sgr0)  Display this message and exit
    $(tput bold)-t$(tput sgr0)  Comma-separated set of teams to promote data for
    $(tput bold)-a$(tput sgr0)  Promote all teams' data
    $(tput bold)-l$(tput sgr0)  List available teams
    $(tput bold)-p$(tput sgr0)  Promote data. If this option is not selected, data that would be copied or deleted is printed out, but files are not actually changed (dry run)
    $(tput bold)-s$(tput sgr0)  Staging bucket type; options are 'uat' or 'dev' ['uat']

EOF
}

log() {
  echo -e "$(tput bold)$(tput setaf 110)[$(date +'%Y-%m-%d %H:%M:%S')] $*$(tput sgr0)" >&1
}

err() {
  echo -e "$(tput bold)$(tput setaf 203)[$(date +'%Y-%m-%d %H:%M:%S')]: $*$(tput sgr0)" >&2
}

list_teams() {
  echo "$(tput bold)Available teams:$(tput sgr0)"
  echo "${ALL_TEAMS[@]}" | tr ' ' '\n'
}

list_gs_files() {
  bucket_type="${1}"
  source_path="${2#/}/**"

  eval "${bucket_type}_gs_files=\$(gsutil ls -l \"${source_path}\" | awk '{print \$3}' | grep '^gs://')"
}

GREEN_CHECKMARK="✅"
RED_X="❌"

non_empty_check() {
  not_empty_tests=
  source_path="${1#/}/**"

  file_list_sorted=$(gsutil ls -l "${source_path}" | sort -n -k 1 | grep -E '^[0-9[:space:]]')

  while read -r file_size timestamp gs_uri; do
    if [[ $file_size -le 10 ]]; then
      err "Found a file less than or equal to 10 bytes: [$gs_uri]"
      not_empty_tests+="\n$RED_X"
    else
      not_empty_tests+="\n$GREEN_CHECKMARK"
    fi
  done <<< "${file_list_sorted}"
}

metadata_check() {
  metadata_present_tests=
  source_path="${1#/}/**"

  file_list=$(gsutil ls -l "${source_path}" | awk '{print $3}' | grep '^gs://')
  subfolder_paths=$(echo "${file_list}" | grep "MANIFEST.tsv$" | awk -F 'MANIFEST.tsv' '{print $1}')

  # Loops through preprocess and cohort_analysis MANIFEST
  while read -r gs_path; do
    files_to_check=$(echo "${file_list}" | grep $gs_path | grep -v "MANIFEST.tsv$" | sed "s;$gs_path;;")
    staging_manifest=$(gsutil cat "${gs_path}MANIFEST.tsv")
    while read -r file; do
      files_exist=$(echo "$staging_manifest" | grep "$file" || [[ $? == 1 ]])
      if [[ -z "$files_exist" ]]; then
        err "File does not have associated metadata and is absent from MANIFEST: [${gs_path}${file}]"
        metadata_present_tests+="\n$RED_X"
      else
        metadata_present_tests+="\n$GREEN_CHECKMARK"
      fi
    done <<< "${files_to_check}"
  done <<< "${subfolder_paths}"
}

get_manifest_info() {
  bucket_type="${1}"
  manifest_locs="${2}"

  eval "${bucket_type}_manifest_combined="

  # Loops through preprocess and cohort_analysis MANIFEST
  while read -r manifest; do
    eval "${bucket_type}_manifest=\$(gsutil cat \"${manifest}\")"
    eval "${bucket_type}_manifest_contents=\$(echo -e \"\${${bucket_type}_manifest}\" | tail -n +2)"
    eval "${bucket_type}_manifest_combined+=\"\${${bucket_type}_manifest_contents}\n\""

    eval "timestamp_column_no=\$(tr '\t' '\n' < <(echo \"\${${bucket_type}_manifest}\" | head -1) | grep -n \"^timestamp\$\" | cut -d : -f 1)"
    eval "workflow_version_column_no=\$(tr '\t' '\n' < <(echo \"\${${bucket_type}_manifest}\" | head -1) | grep -n \"^workflow_version\$\" | cut -d : -f 1)"
    eval "workflow_column_no=\$(tr '\t' '\n' < <(echo \"\${${bucket_type}_manifest}\" | head -1) | grep -n \"^workflow\$\" | cut -d : -f 1)"

    # Preprocess and cohort analysis - TBD if MANIFEST will only contain "Harmonized PMDBS workflow"
    ## One output from cohort_analysis is in the preprocess MANIFEST, must grab the workflow name that has the most occurrences
    eval "${bucket_type}_workflow=\$(echo \"\${${bucket_type}_manifest}\" | cut -f \"${workflow_column_no}\" | tail -n +2 | sed -r '/^\s*$/d' | sort | uniq -c | sort -nr | head -1 | awk '{print \$2}')"
    eval "${bucket_type}_workflows_length=\$(echo \"\${${bucket_type}_workflow}\" | sed '/^$/d' | wc -l | awk '{print \$1}')"
    if [[ "\${${bucket_type}_workflows_length}" != 1 ]]; then
      log "[WARN] More than one workflow detected in ${bucket_type} manifest"
    fi

    # Variables declared are hard-coded in the report generation below
    eval "current_workflow=\${${bucket_type}_workflow}"
    if [[ "$current_workflow" == "preprocess" ]] || [[ "$current_workflow" == "cohort_analysis" ]]; then
      timestamps=$(eval "echo \"\${${bucket_type}_manifest}\"" | cut -f "$timestamp_column_no" | tail -n +2 | sed -r '/^\s*$/d' | sort | uniq)
      eval "${bucket_type}_${current_workflow}_timestamps=\"${timestamps}\""
      workflow_versions=$(eval "echo \"\${${bucket_type}_manifest}\"" | cut -f "$workflow_version_column_no" | tail -n +2 | sed -r '/^\s*$/d' | sort | uniq)
      eval "${bucket_type}_${current_workflow}_workflow_versions=\"${workflow_versions}\""
    else
      log "[WARN] Workflow name other than 'preprocess' and 'cohort_analysis' detected in ${bucket_type} MANIFEST"
    fi
  done <<< "${manifest_locs}"

  eval "manifest_header=$(head -1 <<< "\${${bucket_type}_manifest}")"
}

gsync() {
  source_path="${1#/}/"
  destination_path="${2#/}/"

  # shellcheck disable=SC2086
  gsutil -m rsync \
    -d \
    -r \
    ${DRY_RUN_ARG} \
    "${source_path}" \
    "${destination_path}"
}

ALL_TEAMS=(cohort team-hafler team-hardy team-jakobsson team-lee team-scherzer team-sulzer team-voet team-wood)

# Default to dry run if promotion is not selected
DRY_RUN_ARG="-n"
# Default to dev staging buckets
STAGING_BUCKET_TYPE='uat'

while getopts "ht:alps:" OPTION; do
  case $OPTION in
    h) usage; exit ;;
    t) read -ra TEAMS <<< "$(echo "${OPTARG}" | tr ',' ' ')" ;;
    a) TEAMS=( "${ALL_TEAMS[@]}" ) ;;
    l) list_teams; exit ;;
    p) DRY_RUN_ARG="" ;;
    s) STAGING_BUCKET_TYPE=${OPTARG} ;;
    \?) usage; exit ;;
  esac
done

if [[ -z "${TEAMS:-}" ]]; then
  usage
  list_teams
  exit
fi

if [[ "${STAGING_BUCKET_TYPE}" != "uat" ]] | [[ "${STAGING_BUCKET_TYPE}" != "dev" ]] ; then
  usage
  err "Staging bucket type must be 'uat' or 'dev'"
  exit 1
fi

# Confirm that the teams provided are valid
for team in "${TEAMS[@]}"; do
  if [[ ! "${ALL_TEAMS[*]}" =~ ${team} ]]; then
    err "Team [${team}] is not one of the available teams"
    list_teams
    exit 1
  fi
done

exec > >(tee -a output.log) 2>&1

# Prepare report
current_timestamp=$(date -u +"%Y-%m-%dT%H-%M-%SZ")
table1_header="| timestamp | all tests passed |\n|---------|---------|\n"
table2_header="| filename | timestamp | not empty test | metadata present test |\n|---------|---------|---------|-------------|\n"

# Run data integrity tests and create report for each team
for team in "${TEAMS[@]}"; do
  log "Starting ${team}"

  staging_bucket=gs://asap-${STAGING_BUCKET_TYPE}-data-${team}
  production_bucket=gs://asap-curated-data-${team}

  # Check that files are not empty and are not less than or equal to 10 bytes (factoring in white space)
  log "Checking files are not empty for ${team}"
  non_empty_check \
    "${staging_bucket}"

  # Check that files have associated metadata and is present in MANIFEST.tsv
  log "Checking files have associated metadata for ${team}"
  metadata_check \
    "${staging_bucket}"

  # Get info on target (prod) environment (AKA old/previous workflow run)
  list_gs_files \
    prod \
    "${production_bucket}"

  prod_sample_list_loc=$(echo "$prod_gs_files" | grep "sample_list.tsv$")
  prod_manifest_loc=$(echo "$prod_gs_files" | grep "MANIFEST.tsv$")

  get_manifest_info \
    prod \
    "${prod_manifest_loc}"

  # Get info on initial (staging) environment (AKA new/current workflow run)
  list_gs_files \
    staging \
    "${staging_bucket}"

  staging_sample_list_loc=$(echo "$staging_gs_files" | grep "sample_list.tsv$")
  staging_manifest_loc=$(echo "$staging_gs_files" | grep "MANIFEST.tsv$")

  get_manifest_info \
    staging \
    "${staging_manifest_loc}"

  # Compare staging and prod files
  staging_files=$(echo "$staging_gs_files" | sort | sed 's|^[^/]*//[^/]*/||')
  prod_files=$(echo "$prod_gs_files" | sort | sed 's|^[^/]*//[^/]*/||')

  new_files=$(comm -23 <(echo "$staging_files") <(echo "$prod_files"))
  if [[ -z "$new_files" ]]; then
    log "No new files"
    new_files="N/A"
  else
    log "New files being prepared to transfer to prod:"
    echo "$new_files"
  fi

  deleted_files=$(comm -13 <(echo "$staging_files") <(echo "$prod_files"))
  if [[ -z "$deleted_files" ]]; then
    log "No deleted files"
    deleted_files="N/A"
  else
    log "Deleted files being prepared to remove from prod:"
    echo "$deleted_files"
  fi

  common_files=$(comm -12 <(echo "$staging_files") <(echo "$prod_files"))
  # Get the full paths of the common files in both staging and prod
  common_files_patterns=$(echo "$common_files" | tr '\n' '|')
  common_files_patterns=${common_files_patterns%|}
  staging_matches=$(echo "$staging_gs_files" | grep -E "$common_files_patterns")
  prod_matches=$(echo "$prod_gs_files" | grep -E "$common_files_patterns")
  mod_files=
  while read -r common_file; do
    staging_match=$(echo "$staging_matches" | grep -E "$common_file")
    prod_match=$(echo "$prod_matches" | grep -E "$common_file")
    # Check if the same file exists in prod_files
    staging_filename=$(basename "$staging_match")
    prod_filename=$(basename "$prod_match")
    if [[ "$staging_filename" == "$prod_filename" ]]; then
      log "Comparing md5 hashes for staging file: ["$staging_match"] and prod file: ["$prod_match"]"
      staging_hash=$(gsutil stat "$staging_match" | grep "Hash (md5)" | awk '{print $3}')
      prod_hash=$(gsutil stat "$prod_match" | grep "Hash (md5)" | awk '{print $3}')
      # Compare the hashes
      if [[ "$staging_hash" != "$prod_hash" ]]; then
        mod_files+="\n$staging_match"
      fi
    fi
  done <<< "${common_files}"
  if [[ -z "$mod_files" ]]; then
    log "No modified files"
    mod_files="N/A"
  else
    log "Modified files being prepared to transfer to prod:"
    echo "$mod_files"
  fi

  # Create report
  staging_files_length=$(echo "$staging_files" | wc -l | awk '{print $1}')
  current_timestamps=$(printf "%s\n" $(printf "%0.s$current_timestamp " $(seq 1 $staging_files_length)))
  unique_not_empty_results=$(echo -e "$not_empty_tests" | sort | uniq | sed '/^$/d' | wc -l | awk '{print $1}')
  unique_metadata_present_results=$(echo -e "$metadata_present_tests" | sort | uniq | sed '/^$/d' | wc -l | awk '{print $1}')
  if [[ "$unique_not_empty_results" == 1 ]] && [[ "$unique_metadata_present_results" == 1 ]]; then
    all_tests_result_status=True
    all_tests_result="$GREEN_CHECKMARK"
  else
    all_tests_result_status=False
    all_tests_result="$RED_X"
  fi
  table1=$(printf "| %s | %s |\n" "$current_timestamp" "$all_tests_result")
  table1_content="$table1_header$table1"

  table2=$(paste -d "|" \
    <(echo -e "$staging_files" | sed '/^$/d') \
    <(echo -e "$current_timestamps" | sed '/^$/d') \
    <(echo -e "$not_empty_tests" | sed '/^$/d') \
    <(echo -e "$metadata_present_tests" | sed '/^$/d') \
    | sed 's/|/ | /g' \
    | sed 's/^/| /; s/$/ |/'
  )
  table2_content="$table2_header$table2"

  table4_content=$(echo -e "$manifest_header\n$staging_manifest_combined" \
    | sed '/^$/d' | tr '\t' '|' | sed 's/|/ | /g' | sed 's/^/| /; s/$/ |/' \
    | sed '1a\
|---------|---------|---------|---------|\
'
)

  table5_content=$(echo -e "$manifest_header\n$prod_manifest_combined" \
    | sed '/^$/d' | tr '\t' '|' | sed 's/|/ | /g' | sed 's/^/| /; s/$/ |/' \
    | sed '1a\
|---------|---------|---------|---------|\
'
)

  cat <<EOF > "$team"_"$staging_workflow"_"$staging_cohort_analysis_workflow_versions"_"$current_timestamp"_data-promotion-report.md
# Info
## Initial environment
**Environment:** [${STAGING_BUCKET_TYPE}]

**Bucket:** $staging_bucket

**Preprocessing timestamp(s):**
$(echo -e "$staging_preprocess_timestamps" | sed 's/^/* /')

**Preprocessing version(s):**
$(echo -e "$staging_preprocess_workflow_versions" | sed 's/^/* /')

**Cohort analysis timestamp:** $staging_cohort_analysis_timestamps

**Cohort analysis version:** $staging_cohort_analysis_workflow_versions

**Harmonized PMDBS workflow version:** ($staging_cohort_analysis_workflow_versions)

**Sample set:** $staging_sample_list_loc

**Tests passed:** $all_tests_result_status

## Target environment
**Environment:** [curated]

**Bucket:** gs://asap-curated-data-${team}

**Preprocessing timestamp(s):**
$(echo -e "$prod_preprocess_timestamps" | sed 's/^/* /')

**Preprocessing version(s):**
$(echo -e "$prod_preprocess_workflow_versions" | sed 's/^/* /')

**Cohort analysis timestamp:** $prod_cohort_analysis_timestamps

**Cohort analysis version:** $prod_cohort_analysis_workflow_versions

**Harmonized PMDBS workflow version:** ($prod_cohort_analysis_workflow_versions)

**Sample set:** $prod_sample_list_loc

**Tests passed:** N/A


# Files changed
## New (i.e. only in staging)
$(echo -e "$new_files")

## Modified
$(echo -e "$mod_files")

## Deleted (i.e. only in prod)
$(echo -e "$deleted_files")


# File tests
### Table 1: Summary
$(echo -e "$table1_content")

### Table 2: Data integrity tests results
$(echo -e "$table2_content")

### Table 3: Definitions
| Term | Definition |
|---------|---------|
| New files | Set of new files (i.e. they didn’t exist in previous runs/workflow versions). |
| Modified files | Set of files that have different checksums. |
| Deleted files | Set of files that no longer exist in this version of the pipeline (expected, not an error in the pipeline). |
| Not_empty test | A test that checks if all files in buckets are empty or less than or equal to 10 bytes in size. |
| Metadata_present test | A test that checks if all files in buckets have an associated metadata. The metadata file (MANIFEST.tsv) is generated in the workflow. |


# Manifest files
### Table 4: Staging manifest info (current run)

$(echo -e "$table4_content")

### Table 5: Prod manifest info (previous run)

$(echo -e "$table5_content")
EOF

  log "Local report located at:"
  readlink -f "$team"_"$staging_workflow"_"$staging_cohort_analysis_workflow_versions"_"$current_timestamp"_data-promotion-report.md

  # Exit script if not all tests passed
  if [[ "$all_tests_result_status" == True ]]; then
    log "All tests have passed for ${team}"
  else
    err "One or more tests on files failed for ${team}"
    exit 1
  fi
done

# Try syncing staging data to production
for team in "${TEAMS[@]}"; do
  staging_bucket=gs://asap-${STAGING_BUCKET_TYPE}-data-${team}
  production_bucket=gs://asap-curated-data-${team}

  log "Promoting [${team}] data to production"
  log "\tStaging bucket:\t\t[${staging_bucket}]"
  log "\tProduction bucket:\t[${production_bucket}]"

  gsync \
    "${staging_bucket}" \
    "${production_bucket}"

  # Move report to target bucket only if promoting data
  if [[ "${DRY_RUN_ARG}"=="" ]]; then
    gsutil mv \
      "$team"_"$staging_workflow"_"$staging_cohort_analysis_workflow_versions"_"$current_timestamp"_data-promotion-report.md \
      "${production_bucket}"
    log "Report located at: ["${production_bucket}"/"$team"_"$staging_workflow"_"$staging_cohort_analysis_workflow_versions"_"$current_timestamp"_data-promotion-report.md]"
  fi
done
