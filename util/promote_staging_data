#!/bin/bash
#
# Run data integrity tests and promote data in staging buckets to production

set -eEuo pipefail

usage() {
cat << EOF

  Run data integrity tests and promote data in staging buckets to production.

  $(tput bold)[CAUTION] This also deletes data in the production bucket that is not found in the staging bucket.$(tput sgr0)

  Usage: $0 [OPTIONS]

  OPTIONS
  ───────
    $(tput bold)-h$(tput sgr0)  Display this message and exit
    $(tput bold)-t$(tput sgr0)  Comma-separated set of teams to promote data for
    $(tput bold)-a$(tput sgr0)  Promote all teams' data
    $(tput bold)-l$(tput sgr0)  List available teams
    $(tput bold)-p$(tput sgr0)  Promote data. If this option is not selected, data that would be copied or deleted is printed out, but files are not actually changed (dry run)
    $(tput bold)-s$(tput sgr0)  Staging bucket type; options are 'uat' or 'dev' ['uat']

EOF
}

log() {
  echo -e "$(tput bold)$(tput setaf 110)[$(date +'%Y-%m-%d %H:%M:%S')] $*$(tput sgr0)" >&1
}

err() {
  echo -e "$(tput bold)$(tput setaf 203)[$(date +'%Y-%m-%d %H:%M:%S')]: $*$(tput sgr0)" >&2
}

list_teams() {
  echo "$(tput bold)Available teams:$(tput sgr0)"
  echo "${ALL_TEAMS[@]}" | tr ' ' '\n'
}

list_gs_files() {
  bucket_type="${1}"
  source_path="${2#/}/**"

  eval "${bucket_type}_gs_files=\$(gsutil ls -l \"${source_path}\" | awk '{print \$3}' | grep '^gs://')"
}

GREEN_CHECKMARK="✅"
RED_X="❌"

non_empty_check() {
  not_empty_tests=
  source_path="${1#/}/**"

  file_list_sorted=$(gsutil ls -l "${source_path}" | sort -n -k 1 | grep -E '^[0-9[:space:]]')

  while read -r file_size timestamp gs_uri; do
    if [[ $file_size -le 10 ]]; then
      err "Found a file less than or equal to 10 bytes: [$gs_uri]"
      not_empty_tests+="\n$RED_X"
    else
      not_empty_tests+="\n$GREEN_CHECKMARK"
    fi
  done <<< "${file_list_sorted}"
}

metadata_check() {
  metadata_present_tests=
  source_path="${1#/}/**"

  file_list=$(gsutil ls -l "${source_path}" | awk '{print $3}' | grep '^gs://')
  subfolder_paths=$(echo "${file_list}" | grep "MANIFEST.tsv$" | awk -F 'MANIFEST.tsv' '{print $1}')

  # Loops through preprocess and cohort_analysis MANIFEST
  while read -r gs_path; do
    files_to_check=$(echo "${file_list}" | grep $gs_path | grep -v "MANIFEST.tsv$" | sed "s;$gs_path;;")
    staging_manifest=$(gsutil cat "${gs_path}MANIFEST.tsv")
    while read -r file; do
      files_exist=$(echo "$staging_manifest" | grep "$file" || [[ $? == 1 ]])
      if [[ -z "$files_exist" ]]; then
        err "File does not have associated metadata and is absent from MANIFEST: [${gs_path}${file}]"
        metadata_present_tests+="\n$RED_X"
      else
        metadata_present_tests+="\n$GREEN_CHECKMARK"
      fi
    done <<< "${files_to_check}"
  done <<< "${subfolder_paths}"
}

gsync() {
  source_path="${1#/}/"
  destination_path="${2#/}/"

  # shellcheck disable=SC2086
  gsutil -m rsync \
    -d \
    -r \
    ${DRY_RUN_ARG} \
    "${source_path}" \
    "${destination_path}"
}

ALL_TEAMS=(cohort team-hafler team-hardy team-jakobsson team-lee team-scherzer team-sulzer team-voet team-wood)

# Default to dry run if promotion is not selected
DRY_RUN_ARG="-n"
# Default to dev staging buckets
STAGING_BUCKET_TYPE='uat'

while getopts "ht:alps:" OPTION; do
  case $OPTION in
    h) usage; exit ;;
    t) read -ra TEAMS <<< "$(echo "${OPTARG}" | tr ',' ' ')" ;;
    a) TEAMS=( "${ALL_TEAMS[@]}" ) ;;
    l) list_teams; exit ;;
    p) DRY_RUN_ARG="" ;;
    s) STAGING_BUCKET_TYPE=${OPTARG} ;;
    \?) usage; exit ;;
  esac
done

if [[ -z "${TEAMS:-}" ]]; then
  usage
  list_teams
  exit
fi

if [[ "${STAGING_BUCKET_TYPE}" != "uat" ]] | [[ "${STAGING_BUCKET_TYPE}" != "dev" ]] ; then
  usage
  err "Staging bucket type must be 'uat' or 'dev'"
  exit 1
fi

# Confirm that the teams provided are valid
for team in "${TEAMS[@]}"; do
  if [[ ! "${ALL_TEAMS[*]}" =~ ${team} ]]; then
    err "Team [${team}] is not one of the available teams"
    list_teams
    exit 1
  fi
done

# Prepare report
current_timestamp=$(date -u +"%Y-%m-%dT%H-%M-%SZ")
table1_header="| filename | timestamp | workflow_version | all_tests_passed | previous_timestamp |\n|---------|---------|---------|---------|---------|\n"
table2_header="| filename | timestamp | not_empty_test | metadata_present_test |\n|---------|---------|---------|---------|\n"

# Run data integrity tests and create report for each time
for team in "${TEAMS[@]}"; do
  log "Starting ${team}"

  staging_bucket=gs://asap-${STAGING_BUCKET_TYPE}-data-${team}
  production_bucket=gs://asap-curated-data-${team}

  # Check that files are not empty and are not less than or equal to 10 bytes (factoring in white space)
  non_empty_check \
    "${staging_bucket}"

  # Check that files have associated metadata and is present in MANIFEST.tsv
  metadata_check \
    "${staging_bucket}"

  # Get info on target (prod) environment (AKA old/previous workflow run)
  list_gs_files \
    prod \
    "${production_bucket}"
  prod_sample_list_loc=$(echo "$prod_gs_files" | grep "sample_list.tsv$")
  prod_manifest_loc=$(echo "$prod_gs_files" | grep "MANIFEST.tsv$")
  prod_timestamps=
  prod_workflow_versions=
  # Loops through preprocess and cohort_analysis MANIFEST in prod
  while read -r manifest_loc; do
    prod_manifest=$(gsutil cat "$manifest_loc")
    timestamp_column_no=$(tr '\t' '\n' < <(echo "$prod_manifest" | head -1) | grep -n "^timestamp$" | cut -d : -f 1)
    workflow_version_column_no=$(tr '\t' '\n' < <(echo "$prod_manifest" | head -1) | grep -n "^workflow_version$" | cut -d : -f 1)
    workflow_column_no=$(tr '\t' '\n' < <(echo "$prod_manifest" | head -1) | grep -n "^workflow$" | cut -d : -f 1)
    #workflow_release_column_no=$(tr '\t' '\n' < <(echo "$prod_manifest" | head -1) | grep -n "^workflow_release$" | cut -d : -f 1)

    prod_timestamps+=$(echo "$prod_manifest" | cut -f "$timestamp_column_no" | tail -n+2 | sed -r '/^\s*$/d' | sort | uniq)
    prod_workflow_versions+=$(echo "$prod_manifest" | cut -f "$workflow_version_column_no" | tail -n+2 | sed -r '/^\s*$/d' | sort | uniq)
    prod_workflows=$(echo "$prod_manifest" | cut -f "$workflow_column_no" | tail -n+2 | sed -r '/^\s*$/d' | sort | uniq)
    #prod_workflow_releases=$(echo "$prod_manifest" | cut -f "$workflow_release_column_no" | tail -n+2 | sed -r '/^\s*$/d' | sort | uniq)
  done <<< "${prod_manifest_loc}"
  prod_timestamp=$(echo "$prod_timestamps" | head -1)
  prod_workflow_version=$(echo "$prod_workflow_versions" | head -1)
  prod_workflows_length=$(echo "$prod_workflows" | wc -l | awk '{print $1}')
  prod_workflow=$(echo "$prod_workflows" | head -1)
  #prod_workflow_release=$(echo "$prod_workflow_releases" | head -1)
  if [[ "$prod_workflows_length" != 1 ]]; then
    log "[WARN] More than one workflow detected in prod/previous manifest"
  fi

  # Get info on initial (staging) environment (AKA new/current workflow run)
  list_gs_files \
    staging \
    "${staging_bucket}"
  staging_sample_list_loc=$(echo "$staging_gs_files" | grep "sample_list.tsv$")
  staging_manifest_loc=$(echo "$staging_gs_files" | grep "MANIFEST.tsv$")
  staging_timestamps=
  staging_workflow_versions=
  # Loops through preprocess and cohort_analysis MANIFEST in staging
  while read -r manifest_loc; do
    staging_manifest=$(gsutil cat "$manifest_loc")
    timestamp_column_no=$(tr '\t' '\n' < <(echo "$staging_manifest" | head -1) | grep -n "^timestamp$" | cut -d : -f 1)
    workflow_version_column_no=$(tr '\t' '\n' < <(echo "$staging_manifest" | head -1) | grep -n "^workflow_version$" | cut -d : -f 1)
    workflow_column_no=$(tr '\t' '\n' < <(echo "$staging_manifest" | head -1) | grep -n "^workflow$" | cut -d : -f 1)
    #workflow_release_column_no=$(tr '\t' '\n' < <(echo "$staging_manifest" | head -1) | grep -n "^workflow_release$" | cut -d : -f 1)

    staging_timestamps+=$(echo "$staging_manifest" | cut -f "$timestamp_column_no" | tail -n+2 | sed -r '/^\s*$/d' | sort | uniq)
    staging_workflow_versions+=$(echo "$staging_manifest" | cut -f "$workflow_version_column_no" | tail -n+2 | sed -r '/^\s*$/d' | sort | uniq)
    staging_workflows=$(echo "$staging_manifest" | cut -f "$workflow_column_no" | tail -n+2 | sed -r '/^\s*$/d' | sort | uniq)
    #staging_workflow_releases=$(echo "$staging_manifest" | cut -f "$workflow_release_column_no" | tail -n+2 | sed -r '/^\s*$/d' | sort | uniq)
  done <<< "${staging_manifest_loc}"
  staging_timestamp=$(echo "$staging_timestamps" | head -1)
  staging_workflow_version=$(echo "$staging_workflow_versions" | head -1)
  staging_workflows_length=$(echo "$staging_workflows" | wc -l | awk '{print $1}')
  staging_workflow=$(echo "$staging_workflows" | head -1)
  #staging_workflow_release=$(echo "$staging_workflow_releases" | head -1)
  if [[ "$staging_workflows_length" != 1 ]]; then
    log "[WARN] More than one workflow detected in staging/current manifest"
  fi

  # Compare staging and prod files
  staging_files=$(echo "$file_list" | sort | awk -F/ '{print $NF}')
  prod_files=$(echo "$prod_gs_files" | sort | awk -F/ '{print $NF}')
  new_files=$(comm -23 <(echo "$staging_files") <(echo "$prod_files"))
  deleted_files=$(comm -13 <(echo "$staging_files") <(echo "$prod_files"))
  common_files=$(comm -12 <(echo "$staging_files") <(echo "$prod_files"))
  # Get the full paths of the common files in both staging and prod
  common_files_patterns=$(echo "$common_files" | tr '\n' '|')
  common_files_patterns=${common_files_patterns%|}
  staging_matches=$(echo "$file_list" | grep -E "$common_files_patterns")
  prod_matches=$(echo "$prod_gs_files" | grep -E "$common_files_patterns")
  mod_files=
  while read -r staging_file; do
    staging_filename=$(basename "$staging_file")
    # Check if the same file exists in prod_files
    while read -r prod_file; do
      prod_filename=$(basename "$prod_file")
      if [[ "$staging_filename" == "$prod_filename" ]]; then
        staging_hash=$(gsutil stat "$staging_file" | grep "Hash (md5)" | awk '{print $3}')
        prod_hash=$(gsutil stat "$prod_file" | grep "Hash (md5)" | awk '{print $3}')
        # Compare the hashes
        if [[ "$staging_hash" != "$prod_hash" ]]; then
          mod_files+="\n$staging_filename"
        fi
      fi
    done <<< "${prod_matches}"
  done <<< "${staging_matches}"

  # Create report
  staging_files_length=$(echo "$staging_files" | wc -l | awk '{print $1}')
  current_timestamps=$(printf "%s\n" $(printf "%0.s$current_timestamp " $(seq 1 $staging_files_length)))
  unique_not_empty_results=$(echo "$not_empty_tests" | sort | uniq | wc -l | awk '{print $1}')
  unique_metadata_present_results=$(echo "$metadata_present_tests" | sort | uniq | wc -l | awk '{print $1}')
  if [[ "$unique_not_empty_results" == 1 ]] && [[ "$unique_metadata_present_results" == 1 ]]; then
    all_tests_result_status=True
    all_tests_result="$GREEN_CHECKMARK"
  else
    all_tests_result_status=False
    all_tests_result="$RED_X"
  fi
  table1=$(paste -d " | " \
    <(echo "$staging_files") \
    <(echo "$current_timestamps") \
    <(echo "$staging_workflow_versions") \
    <(echo "$all_tests_result") \
    <(echo "$staging_timestamps") \
    | sed 's/^/| /; s/$/ |/'
  )
  table1_content="$table1_header$table1"

  table2=$(paste -d " | " \
    <(echo "$staging_files") \
    <(echo "$current_timestamps") \
    <(echo "$not_empty_tests") \
    <(echo "$metadata_present_tests") \
    | sed 's/^/| /; s/$/ |/'
  )
  table2_content="$table2_header$table2"

  cat <<EOF > tmp_report.md
# Info
## Initial environment
Environment: [${STAGING_BUCKET_TYPE}]
Bucket: $staging_bucket
Manifest timestamp: $staging_timestamp
Harmonized PMDBS workflow version: ($staging_workflow_version)
Sample set: $staging_sample_list_loc
Tests passed: $all_tests_result_status

## Target environment
Environment: [curated]
Bucket: gs://asap-curated-data-${team}
Manifest timestamp: $prod_timestamp
Harmonized PMDBS workflow version: ($prod_workflow_version)
Sample set: $prod_sample_list_loc
Tests passed: N/A


# Files changed
## New (i.e. only in staging)
$new_files

## Modified
$mod_files

## Deleted (i.e. only in prod)
$deleted_files


# File tests
Table 1: Summary
$table1_content

Table 2: Data integrity tests results
$table2_content

Table 3: Definitions
| Term | Definition |
|---------|---------|
| New files | Set of new files (i.e. they didn’t exist in previous runs/workflow versions). |
| Modified files | Set of files that have different checksums. |
| Deleted files | Set of files that no longer exist in this version of the pipeline (expected, not an error in the pipeline). |
| Not_empty test | A test that checks if all files in buckets are empty or less than or equal to 10 bytes in size. |
| Metadata_present test | A test that checks if all files in buckets have an associated metadata. The metadata file (MANIFEST.tsv) is generated in the workflow. |
EOF

  pandoc tmp_report.md -o "$team"_"$staging_workflow"_"$staging_workflow_version"_"$current_timestamp"_data-promotion-report.pdf
  rm tmp_report.md
  log "Local report located at:"
  readlink -f "$team"_"$staging_workflow"_"$staging_workflow_version"_"$current_timestamp"_data-promotion-report.pdf

  # Exit script if not all tests passed
  if [[ "$all_tests_result_status" == True ]]; then
    log "All tests have passed for ${team}"
  else
    err "One or more tests on files failed for ${team}"
    exit 1
  fi
done

# Try syncing staging data to production
for team in "${TEAMS[@]}"; do
  staging_bucket=gs://asap-${STAGING_BUCKET_TYPE}-data-${team}
  production_bucket=gs://asap-curated-data-${team}

  log "Promoting [${team}] data to production"
  log "\tStaging bucket:\t\t[${staging_bucket}]"
  log "\tProduction bucket:\t[${production_bucket}]"

  gsync \
    "${staging_bucket}" \
    "${production_bucket}"

  # Move report to target bucket
  gsutil mv \
    "$team"_"$staging_workflow"_"$staging_workflow_version"_"$current_timestamp"_data-promotion-report.pdf \
    "${production_bucket}"

  log "Report located at: ["${production_bucket}"/"$team"_"$staging_workflow"_"$staging_workflow_version"_"$current_timestamp"_data-promotion-report.pdf]"
done
