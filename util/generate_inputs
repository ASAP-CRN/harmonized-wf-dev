#!/usr/bin/env python3

from argparse import ArgumentParser
import pandas as pd
import re
import json
from ast import literal_eval


def main(args):
    run_project_cohort_analysis = args.run_project_cohort_analysis
    cohort_staging_bucket_type = "uat"

    projects = dict()
    for project_tsv in args.project_tsvs:
        project_df = pd.read_csv(project_tsv, delimiter="\t")
        # Currently all samples for a team share an embargo status
        project_embargo_status = list(set(project_df.embargoed))
        if len(project_embargo_status) > 1:
            raise SystemExit(
                f"More than one embargo status found for samples in {project_tsv}; don't know how to handle this."
            )
        else:
            staging_bucket_types = (
                ["dev"] if project_embargo_status[0] is True else ["dev", "uat"]
            )
            # If any of the teams' datasets are embargoed, we'll set the cohort bucket to embargoed also
            if project_embargo_status[0] is True:
                cohort_staging_bucket_type = "dev"
        for index, row in project_df.iterrows():
            project_id = row.project_id
            sample_id = row.ASAP_sample_id
            batch = str(row.batch)
            fastq_R1s = literal_eval(row.fastq_R1s)
            fastq_R2s = literal_eval(row.fastq_R2s)
            fastq_I1s = literal_eval(row.fastq_I1s)
            fastq_I2s = literal_eval(row.fastq_I2s)

            sample = {
                "sample_id": sample_id,
                "fastq_R1s": fastq_R1s,
                "fastq_R2s": fastq_R2s,
                "fastq_I1s": fastq_I1s,
                "fastq_I2s": fastq_I2s,
            }

            if not pd.isna(batch):
                sample["batch"] = batch

            if project_id not in projects:
                staging_data_buckets = [
                    f"gs://asap-{staging_bucket_type}-data-{project_id}"
                    for staging_bucket_type in staging_bucket_types
                ]

                projects[project_id] = {
                    "project_id": project_id,
                    "samples": [sample],
                    "run_project_cohort_analysis": run_project_cohort_analysis,
                    "raw_data_bucket": f"gs://asap-raw-data-{project_id}",
                    "staging_data_buckets": staging_data_buckets,
                }
            else:
                projects[project_id]["samples"].append(sample)

    projects = [project for project in projects.values()]

    with open(args.inputs_template, "r") as f:
        inputs_json = json.load(f)

    projects_key = [
        key for key in inputs_json.keys() if re.search(r"^[^\.]*\.projects$", key)
    ]

    if len(projects_key) != 1:
        raise SystemExit(
            f"Failed to find projects key in the inputs template\nProjects keys found: {projects_key}\nAvailable keys: {inputs_json.keys()}"
        )
    else:
        projects_key = projects_key[0]

        inputs_json[projects_key] = projects

    cohort_staging_data_buckets = [
        f"gs://asap-{cohort_staging_bucket_type}-data-cohort"
    ]
    inputs_json[
        "harmonized_pmdbs_analysis.cohort_staging_data_buckets"
    ] = cohort_staging_data_buckets

    output_file = f"{args.output_file_prefix}.{cohort_staging_bucket_type}.json"
    with open(output_file, "w") as f:
        json.dump(inputs_json, f)
    print(f"Wrote input JSON file: {output_file}")


if __name__ == "__main__":
    parser = ArgumentParser(
        description="Given a TSV of sample information, generate an inputs JSON"
    )

    parser.add_argument(
        "-p",
        "--project-tsv",
        dest="project_tsvs",
        type=str,
        action="append",
        required=True,
        help="Project TSV including information for samples present in the project; columns project_id, sample_id, batch, fastq_path. Can provide one per project, or include all samples in a single TSV.",
    )
    parser.add_argument(
        "-i",
        "--inputs-template",
        type=str,
        required=True,
        help="Template JSON file to add project information to (projects will be added at the *.projects key).",
    )
    parser.add_argument(
        "-c",
        "--run-project-cohort-analysis",
        action="store_true",
        required=False,
        help="Run project-level cohort analysis. This will be set for all projects included in the cohort.",
    )
    parser.add_argument(
        "-o",
        "--output-file-prefix",
        required=False,
        default="inputs.json",
        help="Prefix for output JSON file to write workflow inputs to.",
    )

    args = parser.parse_args()

    main(args)
